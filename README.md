R codes used in the article titled: G-computation for increasing performances of randomized clinical trials with binary response
================

## Description

In the article (DOI. XX), we investigated by simulations the performances of G-computation according various machine learning algorithms in the context of randomized clinical trials with a 1:1 allocation ratio, a superiority design and binary outcomes. 

## Data generation

Two distinct scenarios were simulated. In the provided code we consider the complex scenario where the estimation of the outcome model was not straightforward. In the article, we considered different marginal treatment effects and sample sizes. For all the scenarios 10,000 data sets were generated.

[DAGcomplexe.pdf](https://github.com/user-attachments/files/17360824/DAGcomplexe.pdf)

| **Variable** | **Role in the Study** | **Distribution** |
|--------------|-----------------------|------------------|
| Xâ‚           | Continuous covariate  | ð“(0, 1)          |
| Xâ‚‚           | Continuous covariate  | ð“(Î²â‚€ + Î²â‚ * Xâ‚, 1)  |
| Xâ‚ƒ           | Continuous covariate  | ð“(Î²â‚€ - Î²â‚ * Xâ‚ - Î²â‚‚ * Xâ‚‚, 1) |
| Xâ‚„           | Continuous covariate  | ð“(0, 1)          |
| Xâ‚…           | Binary covariate      | ðŸ™{ð“(0, 1) > 0.67} (i.e., prevalence ~ 25%)  |
| Xâ‚†           | Binary covariate      | ðŸ™{ð“(Î²â‚€ - Î²â‚ * Xâ‚„, 1) > -0.40} (i.e., prevalence ~ 50%) |
| Xâ‚‡           | Continuous covariate  | ð“(Î²â‚€ - Î²â‚ * Xâ‚…, 1) |
| Xâ‚ˆ           | Binary covariate      | ðŸ™{ð“(Î²â‚€ + Î²â‚ * Xâ‚†, 1) > -0.80} (i.e., prevalence ~ 75%) |
| Xâ‚‰           | Continuous covariate  | ð“(Î²â‚€ + Î²â‚ * Xâ‚‡, 1) |
| Xâ‚â‚€          | Continuous covariate  | ð“(0, 1)          |
| Xâ‚â‚          | Binary covariate      | ðŸ™{ð“(Î²â‚€ + Î²â‚ * Xâ‚ˆ, 1) > 0.84} (i.e., prevalence ~ 25%) |
| Xâ‚â‚‚          | Continuous covariate  | ð“(Î²â‚€ - Î²â‚ * Xâ‚â‚ - Î²â‚‚ * Xâ‚â‚€, 1) |
| Xâ‚â‚ƒ          | Continuous covariate  | ð“(Î²â‚€ - Î²â‚ * Xâ‚â‚, 1) |
| Xâ‚â‚„          | Continuous covariate  | ð“(0, 1)          |
| Xâ‚â‚…          | Binary covariate      | ðŸ™{ð“(0, 1) > 0.67} (i.e., prevalence ~ 25%) |
| Xâ‚â‚†          | Binary covariate      | ðŸ™{ð“(0, 1) > 0.67} (i.e., prevalence ~ 25%) |
| Xâ‚â‚‡          | Continuous covariate  | ð“(0, 1)          |
| A            | Binary treatment arm   | ðŸ™{ð“(0, 1) > 0} (i.e., a 1:1 randomized clinical trial) |
| Y            | Binary outcome        | ð“‘(n, p = logistic(Î²â‚‚ + Î²â‚ƒ * ðŸ™{Xâ‚‚ > -0.44} - Î²â‚ƒ * Xâ‚ƒ + (Î²â‚ƒ / 2) * Xâ‚ƒÂ² + Î²â‚ƒ * Xâ‚… + Î²â‚ƒ * Xâ‚† + Î²â‚ƒ * Xâ‚‰ + (Î²â‚ƒ / 2) * Xâ‚â‚€Â² - Î²â‚ƒ * Xâ‚â‚‚ - Î²â‚ƒ * (Xâ‚â‚ƒ > -0.55) + Î²â‚ƒ * Xâ‚â‚„ + Î²â‚ƒ * Xâ‚â‚… + (Î²â‚ƒ / 2) * A * Xâ‚â‚„ + Î²â‚„ * A) |
* ðŸ™{a} = 1 if the condition a is true and 0 otherwise; ð“(Î¼, Ïƒ) represents a Gaussian distribution with mean at Î¼ and standard deviation at Ïƒ; ð“‘(n, p) represents a Binomial distribution with a size n and probability of success p. The regression coefficients were: Î²â‚€ = -0.4, Î²â‚ = log(2), Î²â‚‚ = -2, Î²â‚ƒ = log(2), and Î²â‚„ = log(3); log(1.5); log(0.9729) to obtain mOR values of 1.9; 1.3; 1.00 respectively.



## Machine learning techniques

We considered several models and algorithms (learners) to fit the outcome model. All analyses were performed using R version 4.3.0, using the caret package with a tuning grid of length equal to 20. Below an overview of the learners used:

â€¢ Lasso logistic regression. L1 regularization allows for the selection of the predictors. To establish a flexible model, we considered all possible interactions between the treatment arm A and the covariates X. Additionally, we used B-splines for the continuous covariates. The glmnet package was used. The penalization of the L1 norm was the only tuning parameter.
â€¢ Elasticnet logistic regression. This approach mirrors the logistic regression mentioned earlier but incorporates both the L1 and the L2 regularizations.
â€¢ Neural network. We chose one hidden layer, which represents one of the most common network architectures. Its size constitutes the tuning parameter. The nnet package was used.
â€¢ Support vector machine. To relax the linear assumption, we opted for the radial basis function kernel. The svmRadial function of the kernlab package was used. It requires two tuning parameters: the cost penalty of miss-classification and the flexibility of the classification.
â€¢ Super learner. We also tested a super learner with the ensemble of the previous ML techniques. Super learner consisted in a weighting average of the learner-specific predictions by using a weighted linear predictor. In alignment with our previous choices,17 we estimated the weights by maximizing the average AUC through a 20-fold cross-validation. We used the SuperLearner package.

``` r
install.packages("test")
# Output test
#> Likelihood ratio test=70.5213 on 3 df, p=0
#> n=1000, number of events=369
```

This code is a specific case of the complex scenario for n=200 and a mOR=1.9